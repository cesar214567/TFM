@article{PIMENTEL2016744,
	title = {Sharp Hessian integrability estimates for nonlinear elliptic equations: An asymptotic approach},
	journal = {Journal de Mathématiques Pures et Appliquées},
	volume = {106},
	number = {4},
	pages = {744-767},
	year = {2016},
	issn = {0021-7824},
	doi = {https://doi.org/10.1016/j.matpur.2016.03.010},
	url = {https://www.sciencedirect.com/science/article/pii/S0021782416300101},
	author = {Edgard A. Pimentel and Eduardo V. Teixeira},
	keywords = {Fully nonlinear elliptic equations, Regularity theory, A priori  estimates},
	abstract = {We establish sharp W2,p regularity estimates for viscosity solutions of fully nonlinear elliptic equations under minimal, asymptotic assumptions on the governing operator F. By means of geometric tangential methods, we show that if the recession of the operator F – formally given by F⁎(M):=∞−1F(∞M) – is convex, then any viscosity solution to the original equation F(D2u)=f(x) is locally of class W2,p, provided f∈Lp, p>d, with appropriate universal estimates. Our result extends to operators with variable coefficients and in this setting they are new even under convexity of the frozen coefficient operator, M↦F(x0,M), as oscillation is measured only at the recession level. The methods further yield BMO regularity of the Hessian, provided the source lies in that space. As a final application, we establish the density of W2,p solutions within the class of all continuous viscosity solutions, for generic fully nonlinear operators F. This result gives an alternative tool for treating common issues often faced in the theory of viscosity solutions.
	Résumé
	Dans cet article on établit la régularité W2,p des solutions de viscosité des équations elliptiques complètement nonlinéaires. On utilise une méthode géométrique tangentielle, qui est basée sur la notion de fonction de recession d'un opérateur arbitraire F – formellement donnée par F⁎(M):=∞−1F(∞M). De plus, on examine le problème de l'opérateur avec coefficients variables, et on étudie la régularité des solutions dans l'espace BMO. La dernière partie traite d'un résultat concernant la densité des solutions W2,p dans la classe des solutions de viscosité continue.}
}

@article{WorldHealthOrganization2024,
   author = {World Health Organization},
   title = {Monitoring health for the SDGs, Sustainable Development Goals},
   year = {2024},
   journal = {World Health Organization Journal}
}
@article{Bisca2024,
   author = {Paul M Bisca and Vu Chau and Paolo Dudine and Raphael A Espinoza and Jean-Marc Fournier and Pierre Guérin and Niels-Jakob H Hansen and Jorge Salas},
   doi = {10.5089/9798400288470.087.A001},
   issue = {009},
   journal = {Departmental Papers},
   keywords = {Criminology,Economics, Finance, Business and Management,Illegal Behavior and the Enforcement of Law,Social Science},
   month = {11},
   publisher = {International Monetary Fund},
   title = {Violent Crime and Insecurity in Latin America and the Caribbean – A Macroeconomic Perspective},
   volume = {2024},
   url = {https://www.elibrary.imf.org/view/journals/087/2024/009/article-A001-en.xml},
   year = {2024}
}
@techReport{INEGI2024,
   author = {INEGI},
   institution = {INEGI},
   pages = {0-32},
   title = {ENCUESTA NACIONAL DE VICTIMIZACIÓN Y PERCEPCIÓN SOBRE SEGURIDAD PÚBLICA (ENVIPE) 2024},
   url = {https://www.inegi.org.mx/contenidos/saladeprensa/boletines/2024/ENVIPE/ENVIPE_24.pdf},
   year = {2024}
}

@article{Orozco2021,
   abstract = {El reconocimiento de acciones en videos es actualmente un tema de interés en el área de la visión por computador, debido a potenciales aplicaciones como: indexación multimedia, vigilancia en espacios públicos, entre otras. Los mecanismos de atención se han convertido en un concepto muy importante dentro del enfoque de aprendizaje profundo, su operación intenta imitar la capacidad visual de las personas que les permite enfocar su atención en partes relevantes de una escena para extraer información importante. En este artículo proponemos un mecanismo de atención suave adaptado para degradar la arquitectura CNN–LSTM. Primero, una red neuronal convolucional VGG16 extrae las características del video de entrada. Para llevar a cabo las fases de entrenamiento y prueba, usamos los conjuntos de datos HMDB-51 y UCF-101. Evaluamos el desempeño de nuestro sistema usando la precisión como métrica de evaluación, obteniendo 40,7 % (enfoque base), 51,2 % (con atención) para HMDB-51 y 75,8 % (enfoque base), 87,2 % (con atención) para UCF-101.},
   author = {Carlos Ismael Orozco and María Elena Buemi and Julio Jacobo Berlles},
   doi = {10.37537/REV.ELEKTRON.5.1.130.2021},
   issue = {1},
   journal = {Elektron},
   month = {6},
   pages = {37-44},
   publisher = {Facultad de Ingenieria del la Universidad de Buenos Aires},
   title = {CNN–LSTM con mecanismo de atención suave para el reconocimiento de acciones humanas en videos},
   volume = {5},
   year = {2021}
}
@article{Kulkarni2021,
   abstract = {Dealing with imbalanced data is a prevalent problem while performing classification on the datasets. Many times, this problem contributes to bias while making decisions or implementing policies. Thus, it is vital to understand the factors which causes imbalance in the data (or class imbalance). Such hidden biases and imbalances can lead to data tyranny, and a major challenge to a data democracy. In this chapter, two essential statistical elements are resolved: the degree of class imbalance and the complexity of the concept, solving such issues helps in building the foundations of a data democracy. Further, statistical measures which are appropriate in these scenarios are discussed and implemented on a real-life dataset (car insurance claims). In the end, popular data-level methods such as Random Oversampling, Random Undersampling, SMOTE, Tomek Link, and others are implemented in Python, and their performance is compared.},
   author = {Ajay Kulkarni and Feras A Batarseh and Deri Chong},
   keywords = {Complexity of the Concept,Degree of Class Imbalance,Imbalanced Data,Statistical Assessment Metrics,Undersampling and Oversampling},
   title = {Chapter 5: Foundations of Data Imbalance and Solutions for a Data Democracy},
   year = {2021},
   journal = {ArXiv}

}

@article{Marois2021,
   abstract = {Closed-circuit television (CCTV) surveillance units largely rely on the support of surveillance operators. Although this job is cognitively challenging, few studies have investigated the main human factors improving the ability to detect critical incidents in this context. This study aimed to explore the contribution of individual characteristics and cognitive abilities to performance in a realistic CCTV monitoring simulation. Non-expert participants took part in a surveillance simulation and were screened on several measures of individual differences. Improved detection abilities and quicker speed of detection were related to lower age and to better knowledge of the area, cognitive flexibility, working memory, and visual/threat detection abilities. Moreover, more false alarms were associated with higher goal commitment but with lower working memory, visual/threat detection abilities, and cognitive flexibility. Results highlight the potential to screen for a series of cognitive and non-cognitive skills as part of personnel selection procedures for CCTV centers.},
   author = {Alexandre Marois and Helen M. Hodgetts and Cindy Chamberland and Alexandre Williot and Sébastien Tremblay},
   doi = {10.1002/ACP.3837},
   issn = {10990720},
   issue = {4},
   journal = {Applied Cognitive Psychology},
   keywords = {cognitive skills,human factors,individual differences,personnel selection,security surveillance},
   month = {7},
   pages = {1044-1057},
   publisher = {John Wiley and Sons Ltd},
   title = {Who can best find Waldo? Exploring individual differences that bolster performance in a security surveillance microworld},
   volume = {35},
   year = {2021}
}
@article{Negre2024,
   author = {Pablo Negre and Ricardo S. Alonso and Javier Prieto and Cach N. Dang and Juan Manuel Corchado},
   doi = {10.2139/SSRN.4757631},
   journal = {SSRN Electronic Journal},
   keywords = {Cach N. Dang,Explainable artificial intelligence,Javier Prieto,Juan Manuel Corchado,Pablo Negre,Physical aggression,Ricardo S. Alonso,SSRN,Trustworthy artificial intelligence,Video surveillance,Violence detection},
   month = {3},
   publisher = {Elsevier BV},
   title = {Systematic Mapping Study on Violence Detection in Video by Means of Trustworthy Artificial Intelligence},
   url = {https://papers.ssrn.com/abstract=4757631},
   year = {2024}
}
@article{Negre20242,
   author = {Pablo Negre and Ricardo S. Alonso and Javier Prieto and Oscar Garcia and Juan Manuel Corchado},
   doi = {10.2139/SSRN.4832475},
   journal = {SSRN Electronic Journal},
   keywords = {Javier Prieto,Juan Manuel Corchado,LSTM Layers and Bi-LSTM layers,Long Short Term Memory (LSTM),Manual feature,Oscar Garcia,Pablo Negre,Physical aggression,Pre-trained VGG19,Ricardo S. Alonso,SSRN,Video Surveillance,Violence Detection in Video Models Implementation Using Pre-trained VGG19 Combined With Manual Logic,Violence detection},
   month = {5},
   publisher = {Elsevier BV},
   title = {Violence Detection in Video Models Implementation Using Pre-trained VGG19 Combined With Manual Logic, LSTM Layers and Bi-LSTM layers},
   url = {https://papers.ssrn.com/abstract=4832475},
   year = {2024}
}

@article{Abdali2019,
   abstract = {Detection of a violence event in surveillance systems is playing a significant role in law enforcement and city safety. The effectiveness of violence event detectors measures by the speed of response and the accuracy and the generality over different kind of video sources with a different format. Several studies worked on the violence detection with focus either on speed or accuracy or both but not taking into account the generality over different kind of video sources. In this paper, we proposed a real-time violence detector based on deep-learning methods. The proposed model consists of CNN as a spatial feature extractor and LSTM as temporal relation learning method with a focus on the three-factor (overall generality - accuracy - fast response time). The suggested model achieved 98% accuracy with speed of 131 frames/sec. Comparison of the accuracy and the speed of the proposed model with previous works illustrated that the proposed model provides the highest accuracy and the fastest speed among all the previous works in the field of violence detection.},
   author = {Al Maamoon R. Abdali and Rana F. Al-Tuma},
   doi = {10.1109/SCCS.2019.8852616},
   isbn = {9781728107615},
   journal = {SCCS 2019 - 2019 2nd Scientific Conference of Computer Sciences},
   keywords = {CNN,Deep Learning,LSTM,Smart Cities,Violence Detection},
   month = {3},
   pages = {104-108},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Robust Real-Time Violence Detection in Video Using CNN and LSTM},
   year = {2019}
}
@article{Sharma2021,
   abstract = {Recently, the number of violence-related cases in places such as remote roads, pathways, shopping malls, elevators, sports stadiums, and liquor shops, has increased drastically which are unfortunately discovered only after it's too late. The aim is to create a complete system that can perform real-time video analysis which will help recognize the presence of any violent activities and notify the same to the concerned authority, such as the police department of the corresponding area. Using the deep learning networks CNN and LSTM along with a well-defined system architecture, we have achieved an efficient solution that can be used for real-time analysis of video footage so that the concerned authority can monitor the situation through a mobile application that can notify about an occurrence of a violent event immediately.},
   author = {Sarthak Sharma and B. Sudharsan and Saamaja Naraharisetti and Vimarsh Trehan and Kayalvizhi Jayavel},
   doi = {10.11591/IJECE.V11I4.PP3374-3380},
   issn = {20888708},
   issue = {4},
   journal = {International Journal of Electrical and Computer Engineering},
   keywords = {Deep learning LSTM Mobile application Smart cities Transfer learning Violence detection},
   month = {8},
   pages = {3374-3380},
   publisher = {Institute of Advanced Engineering and Science},
   title = {A fully integrated violence detection system using CNN and LSTM},
   volume = {11},
   year = {2021}
}
@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@misc{datascientest2024lstm,
  author       = {DataScientest},
  title        = {Memoria a largo plazo a corto plazo (LSTM): ¿Qué es?},
  year         = {2024},
  url          = {https://datascientest.com/es/memoria-a-largo-plazo-a-corto-plazo-lstm},
  note         = {Accedido el 13 de abril de 2025}
}

@article{Donahue2016,
  author       = {Jeff Donahue and
                  Lisa Anne Hendricks and
                  Sergio Guadarrama and
                  Marcus Rohrbach and
                  Subhashini Venugopalan and
                  Kate Saenko and
                  Trevor Darrell},
  title        = {Long-term Recurrent Convolutional Networks for Visual Recognition
                  and Description},
  journal      = {CoRR},
  volume       = {abs/1411.4389},
  year         = {2014},
  url          = {http://arxiv.org/abs/1411.4389},
  eprinttype    = {arXiv},
  eprint       = {1411.4389},
  timestamp    = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/DonahueHGRVSD14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Sudhakaran2017,
   abstract = {Developing a technique for the automatic analysis of surveillance videos in order to identify the presence of violence is of broad interest. In this work, we propose a deep neural network for the purpose of recognizing violent videos. A convolutional neural network is used to extract frame level features from a video. The frame level features are then aggregated using a variant of the long short term memory that uses convolutional gates. The convolutional neural network along with the convolutional long short term memory is capable of capturing localized spatio-temporal features which enables the analysis of local motion taking place in the video. We also propose to use adjacent frame differences as the input to the model thereby forcing it to encode the changes occurring in the video. The performance of the proposed feature extraction pipeline is evaluated on three standard benchmark datasets in terms of recognition accuracy. Comparison of the results obtained with the state of the art techniques revealed the promising capability of the proposed method in recognizing violent videos.},
   author = {Swathikiran Sudhakaran and Oswald Lanz},
   doi = {10.1109/AVSS.2017.8078468},
   isbn = {9781538629390},
   journal = {2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2017},
   month = {10},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Learning to detect violent videos using convolutional long short-term memory},
   year = {2017}
}


@article{Mann2021,
   author = {Patel Mann},
   doi = {10.48550/arXiv.2107.07578},
   journal = {charotar university of science and technology},
   month = {7},
   title = {Real-Time Violence Detection Using CNN-LSTM},
   url = {https://www.researchgate.net/publication/353330450_Real-Time_Violence_Detection_Using_CNN-LSTM},
   year = {2021}
}



////////////////////////////sadasdsadsadsadsad
@article{OMS2014,
   abstract = {Definición de la violencia La Organización Mundial de la Salud define la violencia como: El uso intencional de la fuerza o el poder físico, de hecho o como amenaza, contra uno mismo, otra persona o un grupo o comunidad, que cause o tenga muchas probabilidades de causar lesiones, muerte, daños psicológicos, trastornos del desarrollo o privaciones. Tipos de violencia La clasificación de la OMS, divide la violencia en tres categorías generales, según las características de los que cometen el acto de violencia:-la violencia autoinfligida (comportamiento suicida y autolesiones),-la violencia interpersonal (violencia familiar, que incluye menores, pareja y ancianos; así como violencia entre personas sin parentesco),-la violencia colectiva (social, política y económica). La naturaleza de los actos de violencia puede ser: física, sexual, psíquica, lo anteriores incluyen privaciones o descuido. La violencia se presenta en distintos ámbitos, por ejemplo, la violencia en el trabajo, que incluye no sólo el maltrato físico sino también psíquico. Muchos trabajadores son sometidos al maltrato, al acoso sexual, a amenazas, a la intimidación y otras formas de violencia psíquica. En investigaciones efectuadas en el Reino Unido se ha comprobado que 53% de los empleados han sufrido intimidación en el trabajo, y 78% han presenciado dicho comportamiento. Los actos repetidos de violencia desde la intimidación, el acoso sexual y las amenazas hasta la humillación y el menosprecio de los trabajadores pueden convertirse en casos muy graves por efecto acumulativo. En Suecia, se calcula que tal comportamiento ha sido un factor en 10% a 15% de los suicidios.},
   author = {OMS},
   journal = {Organismo Mundial de la Salud},
   title = {VIOLENCIA Y SALUD MENTAL},
   url = {https://www.uv.mx/psicologia/files/2014/11/violencia-y-salud-mental-oms.pdf},
   year = {2014}
}

@article{moreno2018revisiones,
  title={Revisiones Sistem{\'a}ticas: definici{\'o}n y nociones b{\'a}sicas},
  author={Moreno, Bego{\~n}a and Mu{\~n}oz, Maximiliano and Cuellar, Javier and Domancic, Stefan and Villanueva, Julio},
  journal={Revista cl{\'\i}nica de periodoncia, implantolog{\'\i}a y rehabilitaci{\'o}n oral},
  volume={11},
  number={3},
  pages={184--186},
  year={2018},
  publisher={Sociedad de Periodoncia de Chile. Sociedad de Implantolog{\'\i}a Oral de Chile~…}
}

@book{zobel,
    author    = "Zobel J",
    title     = "Writing for Computer Science",
    year      = "2015",
    publisher = "Springer",
    ?_edition  = "Third Edition"
}

@book{swales,
    author    = "Swales J. M. and Feak C. B.",
    title     = "Academic Writing for Graduate Students",
    year      = "2012",
    publisher = "The university of Michigan Press",
    ?_edition  = "Third Edition"
}


@inproceedings{10.1145/3419635.3419643,
author = {Lan, Xiaojuan and Bai, Juyang and Li, Meng and Li, Jiajun},
title = {Fish Image Classification Using Deep Convolutional Neural Network},
year = {2020},
isbn = {9781450387729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419635.3419643},
doi = {10.1145/3419635.3419643},
abstract = {Scientific research on species composition and geographical distribution of marine
organisms is of great significance to the research of marine resources and the protection
of rare species of marine life. In these studies, divers or underwater robots are
often used to collect biological images, which are then manually classified by relevant
experts. Manual-based classification is not only time-consuming but also prone to
misjudge. Deep learning algorithms have also been applied in this field, but the classification
performance is poor in general, mainly due to the low image quality and the small
number of collected images. In response to this challenging, a fish classification
algorithm based on Inception-V3 is proposed in this paper. First, data augmentation
is realized by scaling, inverting, and panning of original images. Then transfer learning
method is applied to improve the prediction accuracy. Experimental results show that
the proposed method can effectively improve the classification accuracy, reaching
about 89\% for fish species.},
booktitle = {Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education},
pages = {18–22},
numpages = {5},
keywords = {transfer learning, Inception-V3, fish classification},
location = {Ottawa, ON, Canada},
series = {CIPAE 2020}
}

@inproceedings{10.1145/3325917.3325934,
author = {Manandhar, Nibha and Burris, John W.},
title = {An Application of Image Classification to Saltwater Fish Identification in Louisiana Fisheries},
year = {2019},
isbn = {9781450366359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325917.3325934},
doi = {10.1145/3325917.3325934},
abstract = {Fish identification is a challenge to recreational anglers, but critically important
to the management of fisheries. The state of Louisiana currently provides printed
illustrations of species as the sole aid to anglers for the process of fish identification.
This work describes the application of Google's TensorFlow machine learning library
to the task of fish identification as a case study on the application of the image
classification capabilities. We describe the implementation and results of the project.},
booktitle = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining},
pages = {129–132},
numpages = {4},
keywords = {Fish identification, Image classification, Machine learning},
location = {Houston, TX, USA},
series = {ICISDM 2019}
}

@inproceedings{20.500.12724/11174,
author = {Mejía, Rodrigo and Rosales, Gianfranco},
title = {Sistema de detección y clasificación de peces utilizando visión computacional},
year = {2020},
publisher = {Actas del II Congreso Internacional de Ingeniería de Sistemas },
address = {Lima, LIM, Peru},
url = {https://hdl.handle.net/20.500.12724/11174},
doi = {20.500.12724/11174},
abstract = {La gestión de los recursos hidrobiológicos implica tanto el aspecto ecológico a través del equilibrio del ecosistema, como el aspecto económico mediante el control de la cantidad y calidad de los recursos pesqueros producidos en el Perú. En la actualidad, labores relacionadas a esta gestión son realizadas por empresas privadas y entidades del Estado como el Imarpe. La misión de estas es proteger la calidad de los recursos que llegan a los hogares de millones de peruanos. Esta investigación busca desarrollar un sistema para la detección, clasificación y, finalmente, la medición de diversas especies de peces, utilizando técnicas de visión computacional como el algoritmo SURF y redes neuronales convolucionales. Las pruebas, utilizando dos especies de peces, demostraron que la identificación alcanza un nivel de precisión del 90 \% y que la clasificación alcanza una precisión del 80 \%. Estos valores se obtienen bajo determinadas condiciones que se comentan en el desarrollo del artículo.},
booktitle = {Innovando la educación en tecnología},
pages = {127-141},
numpages = {14},
keywords = {Visión por ordenador, Redes neuronales artificiales	, Peces, Artificial neural networks	},
location = {Lima,LIM,Perú}
}

@article{WildFish ,
   abstract = {Fish recognition is an important task to understand the marine ecosystem and biodiversity. It is often challenging to identify fish species in the wild, due to the following difficulties. First, most fish benchmarks are small-scale, which may limit the representation power of machine learning models. Second, the number of fish species is huge, and there may still exist unknown categories in our planet. The traditional classifiers often fail to deal with this open-set scenario. Third, certain fish species are highly-confused. It is often hard to figure out the subtle differences, only by the unconstrained images. Motivated by these facts, we introduce a large-scale Wild-Fish benchmark for fish recognition in the wild. Specifically, we make three contributions in this paper. First, WildFish is the largest image data set for wild fish recognition, to our best knowledge. It consists of 1000 fish categories with 54,459 unconstrained images, allowing to train high-capacity models for automatic fish classification. Second, we propose a novel open-set fish classification task for realistic scenarios, and investigate the open-set deep learning framework with a number of practical designs. Third, we propose a novel fine-grained recognition task, with the guidance of pairwise textual descriptions. Via leveraging the comparison knowledge in the sentence, we design a multi-modal fish net to effectively distinguish two confused categories in a pair. Finally, we release WildFish (https://github.com/PeiqinZhuang/WildFish), in order to bring benefit to more research studies in multimedia and beyond.},
   author = {Peiqin Zhuang and Yali Wang and Yu Qiao},
   city = {New York, NY, USA},
   doi = {10.1145/3240508},
   journal = {Proceedings of the 26th ACM international conference on Multimedia},
   keywords = {Deep Learning,Fine-Grained Recognition,Fish Classification,Open-Set Classification,Vision-Text Modeling},
   month = {10},
   publisher = {ACM},
   title = {WildFish: A Large Benchmark for Fish Recognition in the Wild},
   url = {https://doi.org/10.1145/3240508.3240616},
   year = {2018},
}

@INPROCEEDINGS{8371919,
  author={Chen, Guang and Sun, Peng and Shang, Yi},
  booktitle={2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Automatic Fish Classification System Using Deep Learning}, 
  year={2017},
  volume={},
  number={},
  pages={24-29},
  doi={10.1109/ICTAI.2017.00016}}

@article{Cui2020,
   abstract = {Recently, human being's curiosity has been expanded from the land to the sky and the sea. Besides sending people to explore the ocean and outer space, robots are designed for some tasks dangerous for living creatures. Take the ocean exploration for an example. There are many projects or competitions on the design of Autonomous Underwater Vehicle (AUV) which attracted many interests. Authors of this article have learned the necessity of platform upgrade from a previous AUV design project, and would like to share the experience of one task extension in the area of fish detection. Because most of the embedded systems have been improved by fast growing computing and sensing technologies, which makes them possible to incorporate more and more complicated algorithms. In an AUV, after acquiring surrounding information from sensors, how to perceive and analyse corresponding information for better judgement is one of the challenges. The processing procedure can mimic human being's learning routines. An advanced system with more computing power can facilitate deep learning feature, which exploit many neural network algorithms to simulate human brains. In this paper, a convolutional neural network (CNN) based fish detection method was proposed. The training data set was collected from the Gulf of Mexico by a digital camera. To fit into this unique need, three optimization approaches were applied to the CNN: data augmentation, network simplification, and training process speed up. Data augmentation transformation provided more learning samples; the network was simplified to accommodate the artificial neural network; the training process speed up is introduced to make the training process more time efficient. Experimental results showed that the proposed model is promising, and has the potential to be extended to other underwear objects.},
   author = {Suxia Cui and Yu Zhou and Yonghui Wang and Lujun Zhai},
   doi = {10.1155/2020/3738108},
   journal = {Applied Computational Intelligence and Soft Computing},
   publisher = {Hindawi Limited},
   title = {Fish Detection Using Deep Learning},
   volume = {2020},
   url = {https://dl.acm.org/doi/pdf/10.1145/3419635.3419643},
   year = {2020},
}
@article{ContinousLearning ,
   abstract = {Our research focuses on a new data flow architecture in neural network training called Continuous Neural Network Learning (CNNL) whose main objective is the reduction of data required to train a neural network. In real-world applications, much of the raw data used in deep learning algorithms do not have a large labeled datasets readily available for training. CNNL seeks to allow for more efficient neural network implementations by significantly reducing the necessary size of the labeled dataset and secondarily decreasing the processing and training time required to achieve reasonable accuracy. Not only is a CNNL system shown to be able to achieve impressive results with little tuning on standardized datasets, but the initialization is as low as 150 images. While this research only the first step and requires further refinement for real world application, it proves the potential for a CNNL system.},
   author = {Michael Baucum and Eric Savage and Daniel Belotto and Prannoy Mupparaju and Sayre Jeannet and Carlos W Morato},
   city = {New York, New York, USA},
   doi = {10.1145/3094243},
   journal = {Proceedings of the 2017 International Conference on Deep Learning Technologies  - ICDLT '17},
   keywords = {CCS Concepts Computing methodologies → Machine learning → Machine learning approaches → Neural networks Keywords Neural Network,CNNL,Continuous,Convolutional Neural Network,Deep Learning,Learning,Semi Supervised,Transfer Learning},
   month = {6},
   publisher = {ACM Press},
   title = {Semi-supervised Deep Continuous Learning},
   url = {http://dx.doi.org/10.1145/3094243.3094247},
   year = {2017},
}
@INPROCEEDINGS{ImageNet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  abstract={The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords={},
  doi={10.1109/CVPR.2009.5206848},
  ISSN={1063-6919},
  month={June},}
  
@article{LeCun1989,
   abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
   author = {Y. LeCun and B. Boser and J. S. Denker and D. Henderson and R. E. Howard and W. Hubbard and L. D. Jackel},
   doi = {10.1162/NECO.1989.1.4.541},
   issn = {0899-7667},
   issue = {4},
   journal = {Neural Computation},
   month = {12},
   pages = {541-551},
   publisher = {MIT Press - Journals},
   title = {Backpropagation Applied to Handwritten Zip Code Recognition},
   volume = {1},
   url = {https://ieeexplore.ieee.org/document/6795724},
   year = {1989},
}
@article{LeCun1990,
   abstract = {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1 \% error rate and about a 9\% reject rate on zipcode digits provided by the U.S. Postal Service.},
   author = {Le Cun and Jackel Henderson and Y Le Cun and J S Denker and D Henderson and R E Howard and W Hubbard and L D Jackel},
   journal = {Advances in neural information processing systems 2},
   pages = {396-404},
   title = {Handwritten Digit Recognition with a Back-Propagation Network},
   url = {https://papers.nips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
   year = {1990},
}

@article{Yanmei2020,
   abstract = {This paper proposes an improved convolutional neural network structure which greatly reduces the scale of network training parameters. It uses the global average pooling algorithm instead of the full connection algorithm to improve the LeNet-5 network. The number of convolution kernels is increased, while the number of subsampling layers is reduced to an optimal value. After verification with MINST handwritten Arabic numeral data set, the results show that the improved network training parameters are only 34.8\% of the original, and the recognition accuracy can achieve 99.3\%.},
   author = {He Yanmei and Wang Bo and Zhu Zhaomin},
   city = {New York, NY, USA},
   doi = {10.1145/3443467},
   isbn = {9781450387811},
   journal = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
   keywords = {CCS CONCEPTS  Computing methodologies KEYWORDS convolutional neural network,LeNet-5,digits recognition},
   publisher = {ACM},
   title = {An improved LeNet-5 model for Image Recognition},
   url = {https://doi.org/10.1145/3443467.3443797},
   year = {2020},
}
@thesis{Lopez2010,
   author = {Patricia Lopez},
   city = {Cantabria},
   institution = {Universidad de Cantabria},
   title = {Desarrollo de sistemas de tiempo real basado en componentes utilizando modelos de comportamiento reactivos},
   url = {https://www.tesisenred.net/bitstream/handle/10803/10639/TesisPLM.pdf?sequence=1},
   year = {2010},
}

@article{Yani2019,
   abstract = {Nails are one part of the fingers and toes, by observing the shape and the condition of the nails, health expert can find out information about a person's health. However, this sometimes not realized and ignored by society, even though many diseases that can be seen through the condition of the nails and the shape of the nails are one of the systemic diseases. This research was conducted to detect abnormalities in the nail based on digital images. The detected abnormalities are terry's nails in the hand which can represent systemic diseases, while the method used is the Convolutional Neural Network (CNN) method. This research uses Tensorflow Inception-V3 architecture model with the transfer learning method where the results of the experiments that have been done are obtained with 95.24\% accuracy.},
   author = {Muhamad Yani and Budhi Irawan and Casi Setiningsih},
   doi = {10.1088/1742-6596/1201/1/012052},
   issn = {17426596},
   issue = {1},
   journal = {Journal of Physics: Conference Series},
   keywords = {Convolutional Neural Network,Nail,Terrys nail},
   month = {5},
   publisher = {Institute of Physics Publishing},
   title = {Application of Transfer Learning Using Convolutional Neural Network Method for Early Detection of Terry's Nail},
   volume = {1201},
   url = {https://www.researchgate.net/publication/333593451_Application_of_Transfer_Learning_Using_Convolutional_Neural_Network_Method_for_Early_Detection_of_Terry's_Nail},
   year = {2019},
}

@misc{Simonyan2015,
   abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
   author = {Karen Simonyan and Andrew Zisserman},
   keywords = {()},
   title = {VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION},
   url = {http://www.robots.ox.ac.uk/},
   year = {2015},
}

@misc{Szegedy2014,
   abstract = {We propose a deep convolutional neural network architecture codenamed Inception , which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
   author = {Christian Szegedy and Wei Liu and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
   title = {Going deeper with convolutions},
   year = {2014},
}

@misc{He2015,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{Tan2020,
   abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https: //github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
   author = {Mingxing Tan and Quoc V Le},
   title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
   url = {https://arxiv.org/pdf/1905.11946.pdf},
   year = {2020},
}




//////////////////////////////////////////////
@MISC{neuronas,
    author = {Magiquo},
    title = {AtomicRedes neuronales o imitar al cerebro humano?},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://magiquo.com/wp-content/uploads/2019/11/neurona.png}
}

@MISC{MLP,
    author = {{CodigoFuente}},
    title = {Redes neuronales profundas – Tipos y Características},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.codigofuente.org/redes-neuronales-profundas-tipos-caracteristicas/}
}

@MISC{convoluciones,
    author = {{Diego Calvo}},
    title = {red-neuronal-convolucional},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.diegocalvo.es/red-neuronal-convolucional/}
}


@MISC{CNN-Arquitectura,
    author = {{Diego Calvo}},
    title = {red-neuronal-convolucional-arquitectura},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.diegocalvo.es/red-neuronal-convolucional/red-neuronal-convolucional-arquitectura/}
}

@MISC{LE-NET5,
    author = {{Viet Tra
Jaeyoung Kim
Sheraz Ali Khan
Jongmyon Kim}},
    title = {red-neuronal-convolucional-arquitectura},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/The-LeNet-5-Architecture-a-convolutional-neural-network_fig4_321586653}
}
@MISC{pooling,
    author = {{Muhamad Yani,
    Budhi Irawan,
    Casi Setianingsih}},
    title = {Application of Transfer Learning Using Convolutional Neural Network Method for Early Detection of Terry’s Nail},
    year = {2019},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451}
}

@MISC{transfer-learning,
    author = {{Wenjin Taoa,
    Md Al-Aminb,
    Haodong Chena,
    Ming C. Leua,
    Zhaozheng Yinc,
    Ruwen Qinb}},
    title = {Real-Time Assembly Operation Recognition with Fog Computing and Transfer Learning for Human-Centered Intelligent Manufacturing},
    year = {2020},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/The-architecture-of-our-transfer-learning-model_fig4_342400905}
}



@MISC{modelos,
    author = {{IchiPro}},
    title = {4 modelos de CNN previamente entrenados para usar en visión artificial con aprendizaje por transferencia},
    year = {2020},
    note = {[Online; accessed December 05, 2021]},
    url = {https://ichi.pro/es/4-modelos-de-cnn-previamente-entrenados-para-usar-en-vision-artificial-con-aprendizaje-por-transferencia-9370731228668}
}

@MISC{EfficientNetB0,
    author = {Tashin Ahmed, Noor Hossain Sabab},
    title = {Classification and understanding of cloud structures via satellite images with EfficientUNet},
    year = {2020},
    note = {[Online; accessed December 05, 2021]},
    url = {https://www.researchgate.net/figure/Architecture-of-EfficientNet-B0-with-MBConv-as-Basic-building-blocks_fig4_344410350}
}

@MISC{DataModelos ,
   author = {Orhan Yalcin},
   journal = {Pre-Trained Model Performances},
   title = {Pre-Trained Model Performances.csv},
   url = {https://gist.github.com/ogyalcin/052f2df49b3288e62086aa0e5fd25fcd},
   year = {2020},
}

@article{Redmon2015,
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.48550/arxiv.1506.02640},
eprint = {1506.02640},
file = {:C\:/Users/CESAR/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {779--788},
publisher = {IEEE Computer Society},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {https://arxiv.org/abs/1506.02640v5},
volume = {2016-December},
year = {2015}
}
@article{Alsmadi2022,
author = {Alsmadi, Mutasem K. and Almarashdeh, Ibrahim},
doi = {10.1016/J.JKSUCI.2020.07.005},
file = {:C\:/Users/CESAR/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alsmadi, Almarashdeh - 2022 - A survey on fish classification techniques.pdf:pdf},
issn = {1319-1578},
journal = {Journal of King Saud University - Computer and Information Sciences},
keywords = {Color features,Features extraction,Fish classification algorithms,Image segmentation,Shape features,Texture features},
month = {may},
number = {5},
pages = {1625--1638},
publisher = {Elsevier},
title = {{A survey on fish classification techniques}},
volume = {34},
year = {2022}
}
@article{Overfitting,
author = {Calvo,Ismael },
doi = {67976/1},
journal = {Journal of King Saud University - Computer and Information Sciences},
keywords = {Color features,Features extraction,Fish classification algorithms,Image segmentation,Shape features,Texture features},
month = {may},
number = {5},
pages = {1625--1638},
publisher = {Elsevier},
title = {{A survey on fish classification techniques}},
volume = {34},
year = {2022}
}
@phdthesis{67976/1 ,
author = {Calvo,Ismael},
title = {Algoritmos de aprendizaje automático para detección de fraudes con tarjeta de crédito: Análisis y comparativa },
publisher = {Universidad politécnica de madrid},
address = {Madrid, España},
year = {2021}
}

@misc{yolov5,
  author       = {Glenn Jocher and
                  Ayush Chaurasia and
                  Alex Stoken and
                  Jirka Borovec and
                  NanoCode012 and
                  Yonghye Kwon and
                  Kalen Michael and
                  TaoXie and
                  Jiacong Fang and
                  imyhxy and
                  Lorna and
                  曾逸夫(Zeng Yifu) and
                  Colin Wong and
                  Abhiram V and
                  Diego Montes and
                  Zhiqiang Wang and
                  Cristi Fati and
                  Jebastin Nadar and
                  Laughing and
                  UnglvKitDe and
                  Victor Sonck and
                  tkianai and
                  yxNONG and
                  Piotr Skalski and
                  Adam Hogan and
                  Dhruv Nair and
                  Max Strobel and
                  Mrinal Jain},
  title        = {{ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime 
                   Instance Segmentation}},
  month        = nov,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v7.0},
  doi          = {10.5281/zenodo.7347926},
  url          = {https://doi.org/10.5281/zenodo.7347926}
}

@misc{unidet,
author = {Zhou, Xingyi; Koltun, Vladlen and Krahenbuhl, Philipp },
doi = {2102.13086},
year = {2022},
month = {Apr},
title = {{Simple Multi-dataset Detection}},
url = {https://arxiv.org/pdf/2102.13086.pdf},
}
@article{cascadercnn,
abstract = {In object detection, an intersection over union (IoU) threshold is required
to define positives and negatives. An object detector, trained with low IoU
threshold, e.g. 0.5, usually produces noisy detections. However, detection
performance tends to degrade with increasing the IoU thresholds. Two main
factors are responsible for this: 1) overfitting during training, due to
exponentially vanishing positive samples, and 2) inference-time mismatch
between the IoUs for which the detector is optimal and those of the input
hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is
proposed to address these problems. It consists of a sequence of detectors
trained with increasing IoU thresholds, to be sequentially more selective
against close false positives. The detectors are trained stage by stage,
leveraging the observation that the output of a detector is a good distribution
for training the next higher quality detector. The resampling of progressively
improved hypotheses guarantees that all detectors have a positive set of
examples of equivalent size, reducing the overfitting problem. The same cascade
procedure is applied at inference, enabling a closer match between the
hypotheses and the detector quality of each stage. A simple implementation of
the Cascade R-CNN is shown to surpass all single-model object detectors on the
challenging COCO dataset. Experiments also show that the Cascade R-CNN is
widely applicable across detector architectures, achieving consistent gains
independently of the baseline detector strength. The code will be made
available at https://github.com/zhaoweicai/cascade-rcnn.},
archivePrefix = {arXiv},
arxivId = {1712.00726},
author = {Cai, Zhaowei and Vasconcelos, Nuno},
doi = {10.1109/CVPR.2018.00644},
eprint = {1712.00726},
file = {:C\:/Users/CESAR/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai, Vasconcelos - 2017 - Cascade R-CNN Delving into High Quality Object Detection(3).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {dec},
pages = {6154--6162},
publisher = {IEEE Computer Society},
title = {{Cascade R-CNN: Delving into High Quality Object Detection}},
url = {https://arxiv.org/abs/1712.00726v1},
year = {2017}
}
